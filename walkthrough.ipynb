{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# introduction\n",
    "\n",
    "The Black-Litterman (BL) asset allocation model is pretty amazing. It consists of three brilliant ideas, whose interplay is a brilliant idea on its own. It is (1) a Bayesian econometrics model (2) with a very smart choice of the prior and the data, and (3) an asset allocation model. Neither of the ideas feels out of place, and neither was introduced just for the sake of it.\n",
    "\n",
    "In this notebook, I would like to visualize the Bayesian dimension of the BL model, as pointed out (among others) by [Kolm and Ritter (2017)](https://cims.nyu.edu/~ritter/kolm2017bayesian.pdf). I take the example from He and Litterman (2009), use `Turing.jl` to set up and numerically solve a Bayesian update model, arriving at the same values as in the closed-form solution derived by Black and Litterman. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# model recap\n",
    "Black and Litterman start by pondering how a portfolio manager could incorporate (possibly contradicting) views about expected returns, as coming e.g from different analysts, into the mean-variance optimization problem. Neat! They approach the problem by making a number of assumptions, and derive a closed-form solution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# black-litterman model through bayesian lens\n",
    "Now, how does the Bayesian connection come about? The answer is: by treating expected returns as a random variable, making a prior belief about its distribution, and updating this belief in the face of new data that is the investors' views! The rest is just plugging in the updated estimates into the asset allocation problem.\n",
    "\n",
    "Here are the steps in detail:\n",
    "\n",
    "1. There are several assets, whose market capitalization dictates the 'market', or 'equilibrium' portfolio $w_m$ &ndash; why this portfolio is important will become clear later;\n",
    "2. the covariance matrix $\\Sigma_r$ of asset returns is assumed to be known or at least estimated with very high precision;\n",
    "3. the expected returns $\\mu_r$, however, are a vector-valued random variable, _initially believed_ to follow the (multivariate) Normal distribution with mean $\\Pi$ and covariance $\\Sigma_{\\mu} = \\tau \\Sigma_r$; $\\Pi$ is taken to be the expected returns which would without any other information result in the market portfolio being mean-variance efficient; in other words, given $\\Pi$, $w_m$ is the solution to: \n",
    "    $$\n",
    "        \\underset{w}{\\max} \\ w'\\Pi - \\frac{\\delta}{2} w'\\Sigma_r w\n",
    "    $$\n",
    "    for some value of $\\delta$.\n",
    "4. the views are kind of data points which can tell a little more about the distribution of $\\mu$; the views can be direct or indirect, but always possible to express as \n",
    "    $$\n",
    "        P \\mu = q + \\varepsilon_q,\n",
    "    $$ \n",
    "    where $\\varepsilon$ is a random variable introducing the noise (of course, Normally distributed!)\n",
    "5. finally, the update of the prior belief from (3) is made based on the views in the Bayesian fashion: intuitively, the updated distribution will reflect the views, the stronger so, the more numerous and less noisy they are;\n",
    "6. finally, we don't need the whole updated distribution of $\\mu_r$, but only its mean because, nesting Normal random variables preserves the mean of the innermost variable, and because, bar the covariance, _nothing else matters_ for the mean-variance optimization.\n",
    "\n",
    "Black and Litterman provide a closed-form solution for the mean of the asset returns (which turns out to be the mean of the updated distribution of $\\mu_r$) and their covariance (which turns out to be updated, too). The solution is derived from some simple mathematics without references to prior, conjugates, posterior, likelihoods and all that Bayesian stuff..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# implementation as a bayesian update problem\n",
    "...but where is fun is that? Let's solve the Bayesian problem numerically using the quite amazing `Turing.jl` library, and confirm what I claim above!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, recreate the environment, where `Turing` and other necessary packages are installed, and import them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m\u001b[1m  Activating\u001b[22m\u001b[39m project at `~/projects/black-litterman-bayes`\n"
     ]
    }
   ],
   "source": [
    "using Pkg\n",
    "\n",
    "Pkg.activate(\".\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Method definition ADgradient(ADTypes.AutoForwardDiff{chunksize, T} where T where chunksize, DynamicPPL.LogDensityFunction{V, M, C} where C where M where V) in module DynamicPPLForwardDiffExt at /home/ipozdeev/.julia/packages/DynamicPPL/zkn0i/ext/DynamicPPLForwardDiffExt.jl:16 overwritten in module Essential at /home/ipozdeev/.julia/packages/Turing/JVSRF/src/essential/ad.jl:23.\n",
      "ERROR: Method overwriting is not permitted during Module precompilation. Use `__precompile__(false)` to opt-out of precompilation.\n"
     ]
    }
   ],
   "source": [
    "using Turing, Distributions, MCMCChains"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, let's import some helper function, like the ones fetching the data from He and Litterman (1999)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "nearest_pd_matrix (generic function with 1 method)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# datafeed functions and nearest_pd_matrix\n",
    "include(\"src_julia/datafeed.jl\")\n",
    "include(\"src_julia/utilities.jl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's load data from that paper: the covariance matrix is constructed using the correlation structure from Table 1 and the standard deviations from Table 2; the market portfolio weights are from Table 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "covariance:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "7×7 Matrix{Float64}:\n",
       " 0.0256  0.0159  0.019   0.0223  0.0148  0.0164  0.0147\n",
       " 0.0159  0.0412  0.0334  0.036   0.0132  0.0247  0.0296\n",
       " 0.019   0.0334  0.0615  0.0579  0.0185  0.0388  0.031\n",
       " 0.0223  0.036   0.0579  0.0734  0.0201  0.0421  0.0331\n",
       " 0.0148  0.0132  0.0185  0.0201  0.0441  0.017   0.012\n",
       " 0.0164  0.0247  0.0388  0.0421  0.017   0.04    0.0244\n",
       " 0.0147  0.0296  0.031   0.0331  0.012   0.0244  0.035"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# covariance and market weights\n",
    "Σ_r = get_covariance();\n",
    "\n",
    "# vcv can be numerically non-hermitian, so let's fix it\n",
    "Σ_r = nearest_pd_matrix(Σ_r);\n",
    "\n",
    "# Pi, the mean of expected returns' distribution\n",
    "Π = get_table(2)[:, :Pi] / 100;\n",
    "\n",
    "n_assets = length(Π);\n",
    "\n",
    "println(\"covariance:\"); flush(stdout)\n",
    "display(round.(Σ_r, digits=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also, define parameters as in the paper: \n",
    "  - the market risk aversion $\\delta$ set to 0.025;\n",
    "  - the scaling factor $\\tau$ for the covariance in the prior distribution of the $\\mu_r$ is set to 0.05; \n",
    "  - the variance of the view about the German market $\\omega$ is set via $\\omega/\\tau = 0.021$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# market risk aversion\n",
    "δ = 2.5;\n",
    "\n",
    "# variance scaling for the expected returns prior\n",
    "τ = 0.05;\n",
    "\n",
    "# (relative) view uncertainty\n",
    "omega_through_tau = 0.021;\n",
    "\n",
    "# view uncertainty\n",
    "ω = omega_through_tau * τ;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, define the necessary $P$ and $Q$ matrices for the view about the German market outperforming the rest of Europe by 5%; $P$ is taken from Table 4. Since we are dealing with the single view, it's easier to define $P$ and $Q$ as vectors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "long-short portfolio with the view on german market:\n",
      "[0.0, 0.0, -0.295, 1.0, 0.0, -0.705, 0.0]\n"
     ]
    }
   ],
   "source": [
    "# view that ger outperformce market cap-weighted fra+gbr by 5% (from table 3)\n",
    "p = get_table(4)[:, :p] / 100;\n",
    "q = 0.05;\n",
    "\n",
    "println(\"long-short portfolio with the view on german market:\")\n",
    "println(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's set up a Bayesian model by:\n",
    "\n",
    "1. treating the expected returns of the assets as a random variable with a prior distribution;\n",
    "2. treating the view about the German market as the realization of a random variable connected to the expected returns;\n",
    "3. adjusting the distribution of returns.\n",
    "\n",
    "$$\n",
    "\\begin{gather}\n",
    "    \\mu_r \\sim N(\\Pi, \\tau \\Sigma_r) \\\\\n",
    "    q | \\mu_r \\sim N(P'\\mu_r, \\omega) \\\\\n",
    "    r | \\mu \\sim N(\\mu_r, \\Sigma),\n",
    "\\end{gather}\n",
    "$$\n",
    "\n",
    "where (1) defines the prior distribution of the expected returns means that the expected returns are initially"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "bl (generic function with 2 methods)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# black-litterman bayesian model\n",
    "@model function bl(_q)\n",
    "\n",
    "    global Π, Σ_r, p, ω\n",
    "\n",
    "    # prior on mu\n",
    "    μ_r ~ MvNormal(Π, τ* Σ_r);\n",
    "\n",
    "    # given mu, the views are distributed as:\n",
    "    _q ~ Normal(dot(p, μ_r), sqrt(ω));\n",
    "    \n",
    "    # condition the model on data\n",
    "    return _q\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "...and sample from the posterior:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: Found initial step size\n",
      "│   ϵ = 0.0125\n",
      "└ @ Turing.Inference /home/ipozdeev/.julia/packages/Turing/JVSRF/src/mcmc/hmc.jl:191\n"
     ]
    }
   ],
   "source": [
    "chain = sample(bl(q), NUTS(), 40000, progress = false);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m7×2 DataFrame\u001b[0m\n",
      "\u001b[1m Row \u001b[0m│\u001b[1m μ_bar   \u001b[0m\u001b[1m μ_bar_exog \u001b[0m\n",
      "     │\u001b[90m Float64 \u001b[0m\u001b[90m Float64    \u001b[0m\n",
      "─────┼─────────────────────\n",
      "   1 │     4.3         4.3\n",
      "   2 │     7.6         7.6\n",
      "   3 │     9.3         9.3\n",
      "   4 │    11.0        11.0\n",
      "   5 │     4.5         4.5\n",
      "   6 │     6.9         7.0\n",
      "   7 │     8.1         8.1"
     ]
    }
   ],
   "source": [
    "post_est, _ = describe(chain);\n",
    "\n",
    "μ_bar = round.(post_est[:, :mean] * 100, digits=1);\n",
    "\n",
    "μ_bar_exog = get_table(4)[:, :mu_bar];\n",
    "print(DataFrame(:μ_bar => μ_bar, :μ_bar_exog => μ_bar_exog))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.10.1",
   "language": "julia",
   "name": "julia-1.10"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
