{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# introduction\n",
    "\n",
    "The Black-Litterman (BL) asset allocation model is pretty amazing. It consists of three brilliant ideas, whose interplay is a brilliant idea on its own. It is (1) a Bayesian econometrics model (2) with a very smart choice of the prior and the data, and (3) an asset allocation model. Neither of the ideas feels out of place, and neither was introduced just for the sake of it.\n",
    "\n",
    "In this notebook, I would like to visualize the Bayesian dimension of Black-Litterman that was pointed out (among others) by [Kolm and Ritter (2017)](https://cims.nyu.edu/~ritter/kolm2017bayesian.pdf). I will use a library for Bayesian inference &ndash; `Turing.jl` &ndash; to numerically solve the example in [He and Litterman (1999)](https://papers.ssrn.com/sol3/papers.cfm?abstract_id=334304), and, of course, will arrive at the same values as in the closed-form solution derived by the original inventors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# model recap\n",
    "Black and Litterman start by pondering how a portfolio manager could incorporate (possibly contradicting) views about expected returns (e.g from different analysts) into the mean-variance optimization problem. Neat! In a nutshell, here is the answer: \n",
    "- treat the expected returns as a random variable; \n",
    "- make the views are a (conveniently linear) function of that random variable plus some noise; \n",
    "- assume the Normal distribution whenever possible;\n",
    "- suddenly, the linearity and the Normal distribution allows to calculate the mean and variance of asset returns...\n",
    "- ...which the portfolio manager can plug into the mean-variance optimization.\n",
    "\n",
    "To put future academics further to shame, Black and Litterman add market equilibrium and currency hedging to the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# black-litterman model through bayesian lens\n",
    "A distribution parameter is a random variable, you say? Let's assume a prior for it, and the Bayesian connection is palpable &ndash; but what is the data here that can be used to update the prior? The views are! Kolm and Ritter (2017) write:\n",
    "\n",
    "> we suggest thinking of a portfolio manager's forecast as an “observation of the future” in which the measuring device is a rather murky and unreliable crystal ball. Only in this way is it analogous to the noisy measurements in experimental design which much of statistics is designed to model.\n",
    "\n",
    "The views can be just one view, they can be contradicting views, they can be a function of the above random variable, etc. When the prior is updated, the mean and variance of the posterior are used in the Markowitz optimization.\n",
    "\n",
    "Black and Litterman choose the prior super cleverly: they say that without any views, the result of the optimization should be the market portfolio, in which the assets are weighted according to their capitalization (this is tantamount to saying that CAPM holds perfectly). \n",
    "\n",
    "Anyway, here is the model in detail:\n",
    "\n",
    "1. To perform the Markowitz optimization, the expected value $\\mu_r$ and the covariance matrix $\\Sigma_r$ of asset returns are needed;\n",
    "2. $\\Sigma_r$ is assumed to be known or at least estimated with very high precision;\n",
    "3. $\\mu_r$, however, are a vector-valued random variable, initially believed to follow the (multivariate) Normal distribution with mean $\\Pi$ and covariance $\\Sigma_{\\mu} = \\tau \\Sigma_r$; $\\Pi$ is taken to be the expected returns which would without any other information result in the market portfolio being mean-variance efficient; in other words, given $\\Pi$, $w_m$ is the solution to: \n",
    "    $$\n",
    "        \\underset{w}{\\max} \\ w'\\Pi - \\frac{\\delta}{2} w'\\Sigma_r w\n",
    "    $$\n",
    "    for some value of $\\delta$.\n",
    "4. the views are data points which can tell a little more about the distribution of $\\mu_r$; the views can be direct or indirect, but always possible to express as \n",
    "    $$\n",
    "        P \\mu_r = q + \\varepsilon_q,\n",
    "    $$ \n",
    "    where $\\varepsilon$ is a random variable introducing the noise (of course, Normally distributed!)\n",
    "5. finally, the update of the prior belief from (3) is made based on the views in the Bayesian fashion: intuitively, the updated distribution will reflect the views, the stronger so, the more numerous and less noisy they are;\n",
    "6. finally, we don't need the whole updated distribution of $\\mu_r$, but only its mean because, nesting Normal random variables preserves the mean of the innermost variable, and because, bar the covariance, _nothing else matters_ for the mean-variance optimization.\n",
    "\n",
    "Black and Litterman provide a closed-form solution for the mean of the asset returns (which turns out to be the mean of the updated distribution of $\\mu_r$) and their covariance (which turns out to be updated, too). The solution is derived from some simple mathematics without references to prior, conjugates, posterior, likelihoods and all that Bayesian stuff..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# implementation as a bayesian update problem\n",
    "...but where is fun is that? Let's solve the Bayesian problem numerically using the quite amazing `Turing.jl` library, and confirm what I claim above!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, recreate the environment, where `Turing` and other necessary packages are installed, and import them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m\u001b[1m  Activating\u001b[22m\u001b[39m project at `~/projects/black-litterman-bayes`\n"
     ]
    }
   ],
   "source": [
    "using Pkg\n",
    "\n",
    "Pkg.activate(\".\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Method definition ADgradient(ADTypes.AutoForwardDiff{chunksize, T} where T where chunksize, DynamicPPL.LogDensityFunction{V, M, C} where C where M where V) in module DynamicPPLForwardDiffExt at /home/ipozdeev/.julia/packages/DynamicPPL/zkn0i/ext/DynamicPPLForwardDiffExt.jl:16 overwritten in module Essential at /home/ipozdeev/.julia/packages/Turing/JVSRF/src/essential/ad.jl:23.\n",
      "ERROR: Method overwriting is not permitted during Module precompilation. Use `__precompile__(false)` to opt-out of precompilation.\n"
     ]
    }
   ],
   "source": [
    "using Turing, Distributions, MCMCChains"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, let's import some helper function, like the ones fetching the data from He and Litterman (1999)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "nearest_pd_matrix (generic function with 1 method)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# datafeed functions and nearest_pd_matrix\n",
    "include(\"src_julia/datafeed.jl\")\n",
    "include(\"src_julia/utilities.jl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's load data from that paper: the covariance matrix is constructed using the correlation structure from Table 1 and the standard deviations from Table 2; the market portfolio weights are from Table 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "covariance:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "7×7 Matrix{Float64}:\n",
       " 0.0256  0.0159  0.019   0.0223  0.0148  0.0164  0.0147\n",
       " 0.0159  0.0412  0.0334  0.036   0.0132  0.0247  0.0296\n",
       " 0.019   0.0334  0.0615  0.0579  0.0185  0.0388  0.031\n",
       " 0.0223  0.036   0.0579  0.0734  0.0201  0.0421  0.0331\n",
       " 0.0148  0.0132  0.0185  0.0201  0.0441  0.017   0.012\n",
       " 0.0164  0.0247  0.0388  0.0421  0.017   0.04    0.0244\n",
       " 0.0147  0.0296  0.031   0.0331  0.012   0.0244  0.035"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# covariance and market weights\n",
    "Σ_r = get_covariance();\n",
    "\n",
    "# vcv can be numerically non-hermitian, so let's fix it\n",
    "Σ_r = nearest_pd_matrix(Σ_r);\n",
    "\n",
    "# Pi, the mean of expected returns' distribution\n",
    "Π = get_table(2)[:, :Pi] / 100;\n",
    "\n",
    "n_assets = length(Π);\n",
    "\n",
    "println(\"covariance:\"); flush(stdout)\n",
    "display(round.(Σ_r, digits=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also, define parameters as in the paper: \n",
    "  - the market risk aversion $\\delta$ set to 0.025;\n",
    "  - the scaling factor $\\tau$ for the covariance in the prior distribution of the $\\mu_r$ is set to 0.05; \n",
    "  - the variance of the view about the German market $\\omega$ is set via $\\omega/\\tau = 0.021$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# market risk aversion\n",
    "δ = 2.5;\n",
    "\n",
    "# variance scaling for the expected returns prior\n",
    "τ = 0.05;\n",
    "\n",
    "# (relative) view uncertainty\n",
    "omega_through_tau = 0.021;\n",
    "\n",
    "# view uncertainty\n",
    "ω = omega_through_tau * τ;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, define the necessary $P$ and $Q$ matrices for the view about the German market outperforming the rest of Europe by 5%; $P$ is taken from Table 4. Since we are dealing with the single view, it's easier to define $P$ and $Q$ as vectors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "long-short portfolio with the view on german market:\n",
      "[0.0, 0.0, -0.295, 1.0, 0.0, -0.705, 0.0]\n"
     ]
    }
   ],
   "source": [
    "# view that ger outperformce market cap-weighted fra+gbr by 5% (from table 3)\n",
    "p = get_table(4)[:, :p] / 100;\n",
    "q = 0.05;\n",
    "\n",
    "println(\"long-short portfolio with the view on german market:\")\n",
    "println(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's set up a Bayesian model by:\n",
    "\n",
    "1. treating the expected returns of the assets as a random variable with a prior distribution;\n",
    "2. treating the view about the German market as the realization of a random variable connected to the expected returns;\n",
    "3. adjusting the distribution of returns.\n",
    "\n",
    "$$\n",
    "\\begin{gather}\n",
    "    \\mu_r \\sim N(\\Pi, \\tau \\Sigma_r) \\\\\n",
    "    q | \\mu_r \\sim N(P'\\mu_r, \\omega) \\\\\n",
    "    r | \\mu \\sim N(\\mu_r, \\Sigma),\n",
    "\\end{gather}\n",
    "$$\n",
    "\n",
    "where (1) defines the prior distribution of the expected returns means that the expected returns are initially"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "blb (generic function with 2 methods)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# black-litterman bayesian model\n",
    "@model function blb(_q)\n",
    "\n",
    "    global Π, Σ_r, p, ω\n",
    "\n",
    "    # prior on mu\n",
    "    μ_r ~ MvNormal(Π, τ* Σ_r);\n",
    "\n",
    "    # given mu, the views are distributed as:\n",
    "    _q ~ Normal(dot(p, μ_r), sqrt(ω));\n",
    "    \n",
    "    # condition the model on data\n",
    "    return _q\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "...and sample from the posterior:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: Found initial step size\n",
      "│   ϵ = 0.025\n",
      "└ @ Turing.Inference /home/ipozdeev/.julia/packages/Turing/JVSRF/src/mcmc/hmc.jl:191\n"
     ]
    }
   ],
   "source": [
    "chain = sample(blb(q), NUTS(), 40000, progress = false);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m7×2 DataFrame\u001b[0m\n",
      "\u001b[1m Row \u001b[0m│\u001b[1m μ_bar   \u001b[0m\u001b[1m μ_bar_paper \u001b[0m\n",
      "     │\u001b[90m Float64 \u001b[0m\u001b[90m Float64     \u001b[0m\n",
      "─────┼──────────────────────\n",
      "   1 │     4.4          4.3\n",
      "   2 │     7.7          7.6\n",
      "   3 │     9.5          9.3\n",
      "   4 │    11.2         11.0\n",
      "   5 │     4.6          4.5\n",
      "   6 │     7.1          7.0\n",
      "   7 │     8.2          8.1"
     ]
    }
   ],
   "source": [
    "post_est, _ = describe(chain);\n",
    "\n",
    "μ_bar = round.(post_est[:, :mean] * 100, digits=1);\n",
    "\n",
    "μ_bar_exog = get_table(4)[:, :mu_bar];\n",
    "print(DataFrame(:μ_bar => μ_bar, :μ_bar_paper => μ_bar_exog))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bar numerical differences, a perfect match! Now, what about the covariance $\\Sigma_r$? To get to it, we need to sample from the posterior of $r$, which can be achieved by rewriting the function slightly (see the [Turing reference](https://turing.ml/dev/docs/using-turing/guide#treating-observations-as-random-variables) for details):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "blb_with_r (generic function with 2 methods)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# black-litterman bayesian model\n",
    "@model function blb_with_r(q, _r)\n",
    "\n",
    "    global Π, Σ_r, p, ω\n",
    "\n",
    "    if _r === missing\n",
    "        # Initialize `q` if missing\n",
    "        _r = Vector{Float64}(undef, n_assets);\n",
    "    end\n",
    "\n",
    "    # prior on mu\n",
    "    μ_r ~ MvNormal(Π, τ* Σ_r);\n",
    "\n",
    "    # given mu, the views are distributed as:\n",
    "    _q ~ Normal(dot(p, μ_r), sqrt(ω));\n",
    "    \n",
    "    # given mu, the returns are distributed as:\n",
    "    _r ~ MvNormal(μ_r, Σ_r);\n",
    "    \n",
    "    # condition the model on data\n",
    "    return q\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: Found initial step size\n",
      "│   ϵ = 0.0125\n",
      "└ @ Turing.Inference /home/ipozdeev/.julia/packages/Turing/JVSRF/src/mcmc/hmc.jl:191\n"
     ]
    }
   ],
   "source": [
    "# sample from the posterior of r\n",
    "chain = sample(blb_with_r(q, missing), NUTS(), 40000, progress = false);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m6×7 DataFrame\u001b[0m\n",
      "\u001b[1m Row \u001b[0m│\u001b[1m _r[1]      \u001b[0m\u001b[1m _r[2]      \u001b[0m\u001b[1m _r[3]      \u001b[0m\u001b[1m _r[4]      \u001b[0m\u001b[1m _r[5]      \u001b[0m\u001b[1m _r[6]      \u001b[0m\u001b[1m _r[7]      \u001b[0m\n",
      "     │\u001b[90m Float64    \u001b[0m\u001b[90m Float64    \u001b[0m\u001b[90m Float64    \u001b[0m\u001b[90m Float64    \u001b[0m\u001b[90m Float64    \u001b[0m\u001b[90m Float64    \u001b[0m\u001b[90m Float64    \u001b[0m\n",
      "─────┼────────────────────────────────────────────────────────────────────────────────────\n",
      "   1 │ -0.278585    0.0476169   0.177344   -0.0659182  -0.228496   -0.0427864   0.0507353\n",
      "   2 │ -0.0997189   0.119443   -0.126084   -0.0130549  -0.0951963  -0.28269     0.0738249\n",
      "   3 │ -0.0381792   0.10803     0.0269905  -0.113583   -0.121041   -0.18265     0.0481404\n",
      "   4 │ -0.278518   -0.199939   -0.200783   -0.321998    0.184968    0.0608774  -0.0611563\n",
      "   5 │ -0.1367      0.0521286   0.0501448   0.155181    0.168805    0.0724417  -0.0674361\n",
      "   6 │  0.0734958   0.433291    0.229726    0.121449    0.202212    0.307832    0.607469"
     ]
    }
   ],
   "source": [
    "r_sim = DataFrame(chain)[!, (n_assets+1+3):(n_assets+1+3+n_assets-1)];\n",
    "\n",
    "print(r_sim[1:6, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Now we can just estimate the covariance matrix of these values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7×7 Matrix{Float64}:\n",
       " 0.0269  0.0167  0.02    0.0236  0.0154  0.0172  0.0153\n",
       " 0.0167  0.0431  0.035   0.0378  0.0139  0.0259  0.031\n",
       " 0.02    0.035   0.0647  0.0608  0.0196  0.0412  0.0328\n",
       " 0.0236  0.0378  0.0608  0.0773  0.0213  0.0446  0.035\n",
       " 0.0154  0.0139  0.0196  0.0213  0.0457  0.0179  0.0128\n",
       " 0.0172  0.0259  0.0412  0.0446  0.0179  0.0422  0.0257\n",
       " 0.0153  0.031   0.0328  0.035   0.0128  0.0257  0.0367"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "Σ_r_post = cov(Matrix(r_sim));\n",
    "\n",
    "display(round.(Σ_r_post, digits=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compared to the original covariance matrix, the posterior is larger in magnitude"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7×7 Matrix{Float64}:\n",
       " 0.0256  0.0159  0.019   0.0223  0.0148  0.0164  0.0147\n",
       " 0.0159  0.0412  0.0334  0.036   0.0132  0.0247  0.0296\n",
       " 0.019   0.0334  0.0615  0.0579  0.0185  0.0388  0.031\n",
       " 0.0223  0.036   0.0579  0.0734  0.0201  0.0421  0.0331\n",
       " 0.0148  0.0132  0.0185  0.0201  0.0441  0.017   0.012\n",
       " 0.0164  0.0247  0.0388  0.0421  0.017   0.04    0.0244\n",
       " 0.0147  0.0296  0.031   0.0331  0.012   0.0244  0.035"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(round.(Σ_r, digits=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "...by &ndash; surprize? &ndash; $(1 + \\tau)$!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7×7 Matrix{Float64}:\n",
       " 1.05  1.05  1.05  1.05  1.04  1.05  1.04\n",
       " 1.05  1.05  1.05  1.05  1.05  1.05  1.05\n",
       " 1.05  1.05  1.05  1.05  1.06  1.06  1.06\n",
       " 1.05  1.05  1.05  1.05  1.06  1.06  1.06\n",
       " 1.04  1.05  1.06  1.06  1.04  1.05  1.06\n",
       " 1.05  1.05  1.06  1.06  1.05  1.06  1.05\n",
       " 1.04  1.05  1.06  1.06  1.06  1.05  1.05"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(round.(Σ_r_post ./ Σ_r, digits=2))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.10.1",
   "language": "julia",
   "name": "julia-1.10"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
