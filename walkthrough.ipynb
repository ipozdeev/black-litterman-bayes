{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# introduction\n",
    "\n",
    "The Black-Litterman (BL) asset allocation model is pretty amazing. It consists of three brilliant ideas, whose interplay is a brilliant idea on its own. It is (1) a Bayesian econometrics model (2) with a very smart choice of the prior and the data, and (3) an asset allocation model. Neither of the ideas feels out of place, and neither was introduced just for the sake of it.\n",
    "\n",
    "In this notebook, I would like to visualize the Bayesian dimension of Black-Litterman that was pointed out (among others) by [Kolm and Ritter (2017)](https://cims.nyu.edu/~ritter/kolm2017bayesian.pdf). I will use a library for Bayesian inference &ndash; `Turing.jl` &ndash; to numerically solve the example in [He and Litterman (1999)](https://papers.ssrn.com/sol3/papers.cfm?abstract_id=334304), and, of course, will arrive at the same values as in the closed-form solution derived by the original inventors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# bayesian econometrics recap\n",
    "In the Bayesian econometrics, randomness is nested: the observed data is assumed to be generated by a stochastic process of some parameters, and the parameters themselves are assumed stochastic! This way, the data produced would be random even if the parameters were fixed and known, but since they aren't, the data becomes... twice as random? Anyway, the Bayesian econometrician starts with a prior belief about the distribution of the parameters, then checks the likelihood of observing the data if that belief is correct, and updates the belief accordingly &ndash; stronger so if the likelihood is low. The end result is the tug-of-war between the prior and the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# black-litterman model recap\n",
    "Black and Litterman ponder how a portfolio manager could incorporate (possibly contradicting) views about expected returns (e.g from different analysts) into the mean-variance optimization problem. Neat! They decide to do the following: \n",
    "- treat the expected returns as a random variable; \n",
    "- make the views a (conveniently linear) function of that random variable plus some noise; \n",
    "- assume the Normal distribution whenever possible;\n",
    "- enjoy a closed-form solution for the mean and covariance of asset returns because of the linearity and gaussianity;\n",
    "- plug the mean and covariance into the mean-variance optimization and call it day...\n",
    "- ...or not, in which case also add market equilibrium and currency hedging to the model (possibly to taunt future academics even more)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# black-litterman model through bayesian lens\n",
    "A distribution parameter is a random variable, you say? Sure sounds Bayesian! The only missing piece is the data that the econometrician would use to update the prior, but the views can play this role. After all, as Kolm and Ritter (2017) write:\n",
    "\n",
    "> we suggest thinking of a portfolio manager's [view] as an “observation of the future” in which the measuring device is a rather murky and unreliable crystal ball. Only in this way is it analogous to the noisy measurements in experimental design which much of statistics is designed to model.\n",
    "\n",
    "The views can be just one view, they can be contradicting views, they can be indirect, i.e. a function of the parameter of interest, etc. &ndash; in other words, behave just like data. \n",
    "\n",
    "After the prior is updated, the mean and variance of the posterior are used in the Markowitz optimization. Modern techniques allow to efficiently sample from the posterior, thus eliminating the need in closed-form solutions and allowing to extend the original setting.\n",
    "\n",
    "Anyway, here is the model in detail:\n",
    "\n",
    "1. To perform the Markowitz optimization, the expected value $\\mu_r$ and the covariance matrix $\\Sigma_r$ of asset returns are needed;\n",
    "2. $\\Sigma_r$ is assumed to be known or at least estimated with very high precision;\n",
    "3. $\\mu_r$, however, are a vector-valued random variable, initially believed to follow the (multivariate) Normal distribution with mean $\\Pi$ and covariance $\\Sigma_{\\mu} = \\tau \\Sigma_r$; $\\Pi$ is taken to be the expected returns which would without any other information result in the market portfolio being mean-variance efficient; this is beyond the scope of this piece, and it is suffices to assume $\\Pi$ is known;\n",
    "4. the views are data points which can tell a little more about the distribution of $\\mu_r$, since they depend on it in the following form: \n",
    "    $$\n",
    "        P \\mu_r = q + \\varepsilon_q,\n",
    "    $$ \n",
    "    where $\\varepsilon$ is a random variable introducing the noise (of course, Normally distributed!)\n",
    "5. finally, the update of the prior belief from (3) is made based on the views in the Bayesian fashion: intuitively, the updated distribution will reflect the views, the stronger so, the more numerous and less noisy they are;\n",
    "6. finally, we don't need the whole updated distribution of $\\mu_r$, but only its mean because nesting Normal random variables preserves the mean of the innermost variable, and because, bar the covariance, _nothing else matters_ for the mean-variance optimization.\n",
    "\n",
    "Black and Litterman provide a closed-form solution for the mean of the asset returns (which turns out to be the mean of the updated distribution of $\\mu_r$) and their covariance (which turns out to be updated, too). The solution is derived from some simple mathematics without references to prior, conjugates, posterior, likelihoods and all that Bayesian stuff..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# implementation as a bayesian update problem\n",
    "...but where is fun is that? Let's solve the problem numerically using the outstanding `Turing.jl`, and confirm that Black-Litterman is essentially Black-Litterman-Bayes!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, recreate the environment, where `Turing` and other necessary packages are installed, and import them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m\u001b[1m  Activating\u001b[22m\u001b[39m project at `~/projects/black-litterman-bayes`\n"
     ]
    }
   ],
   "source": [
    "using Pkg\n",
    "\n",
    "Pkg.activate(\".\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "using Turing, Distributions, MCMCChains"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, let's import some helper function, like the ones fetching the data from He and Litterman (1999)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "nearest_pd_matrix (generic function with 1 method)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# datafeed functions and nearest_pd_matrix\n",
    "include(\"src_julia/datafeed.jl\")\n",
    "include(\"src_julia/utilities.jl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's load data from that paper: the covariance matrix is constructed using the correlation structure from Table 1 and the standard deviations from Table 2; the market portfolio weights are from Table 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "covariance:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "7×7 Matrix{Float64}:\n",
       " 0.0256  0.0159  0.019   0.0223  0.0148  0.0164  0.0147\n",
       " 0.0159  0.0412  0.0334  0.036   0.0132  0.0247  0.0296\n",
       " 0.019   0.0334  0.0615  0.0579  0.0185  0.0388  0.031\n",
       " 0.0223  0.036   0.0579  0.0734  0.0201  0.0421  0.0331\n",
       " 0.0148  0.0132  0.0185  0.0201  0.0441  0.017   0.012\n",
       " 0.0164  0.0247  0.0388  0.0421  0.017   0.04    0.0244\n",
       " 0.0147  0.0296  0.031   0.0331  0.012   0.0244  0.035"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# covariance and market weights\n",
    "Σ_r = get_covariance();\n",
    "\n",
    "# vcv can be numerically non-hermitian, so let's fix it\n",
    "Σ_r = nearest_pd_matrix(Σ_r);\n",
    "\n",
    "# Pi, the mean of expected returns' distribution\n",
    "Π = get_table(2)[:, :Pi] / 100;\n",
    "\n",
    "n_assets = length(Π);\n",
    "\n",
    "println(\"covariance:\"); flush(stdout)\n",
    "display(round.(Σ_r, digits=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also, define parameters as in the paper: \n",
    "  - the scaling factor $\\tau$ for the covariance in the prior distribution of the $\\mu_r$ is set to 0.05; \n",
    "  - the variance of the view about the German market $\\omega$ is set via $\\omega/\\tau = 0.021$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# variance scaling for the expected returns prior\n",
    "τ = 0.05;\n",
    "\n",
    "# (relative) view uncertainty\n",
    "omega_through_tau = 0.021;\n",
    "\n",
    "# view uncertainty\n",
    "ω = omega_through_tau * τ;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, define the necessary $P$ and $Q$ matrices for the view about the German market outperforming the rest of Europe by 5%; $P$ is taken from Table 4. Since we are dealing with the single view, it's easier to define $P$ and $Q$ as vectors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "long-short portfolio with the view on german market:\n",
      "[0.0, 0.0, -0.295, 1.0, 0.0, -0.705, 0.0]\n"
     ]
    }
   ],
   "source": [
    "# view that ger outperformce market cap-weighted fra+gbr by 5% (from table 3)\n",
    "p = get_table(4)[:, :p] / 100;\n",
    "q = 0.05;\n",
    "\n",
    "println(\"long-short portfolio with the view on german market:\")\n",
    "println(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's set up a Bayesian model by:\n",
    "\n",
    "1. treating the expected returns of the assets as a random variable with a prior distribution;\n",
    "2. treating the view about the German market as the realization of a random variable connected to the expected returns;\n",
    "3. adjusting the distribution of returns.\n",
    "\n",
    "$$\n",
    "\\begin{gather}\n",
    "    \\mu_r \\sim N(\\Pi, \\tau \\Sigma_r) \\\\\n",
    "    q | \\mu_r \\sim N(P'\\mu_r, \\omega) \\\\\n",
    "    r | \\mu \\sim N(\\mu_r, \\Sigma).\n",
    "\\end{gather}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "blb (generic function with 2 methods)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# black-litterman bayesian model\n",
    "@model function blb(_q)\n",
    "\n",
    "    global Π, Σ_r, p, ω\n",
    "\n",
    "    # prior on mu\n",
    "    μ_r ~ MvNormal(Π, τ* Σ_r);\n",
    "\n",
    "    # given mu, the views are distributed as:\n",
    "    _q ~ Normal(dot(p, μ_r), sqrt(ω));\n",
    "    \n",
    "    # condition the model on data\n",
    "    return _q\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "...and sample from the posterior:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: Found initial step size\n",
      "│   ϵ = 0.025\n",
      "└ @ Turing.Inference /home/ipozdeev/.julia/packages/Turing/lkUBK/src/mcmc/hmc.jl:191\n"
     ]
    }
   ],
   "source": [
    "chain = sample(blb(q), NUTS(), 40000, progress = false);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m7×2 DataFrame\u001b[0m\n",
      "\u001b[1m Row \u001b[0m│\u001b[1m μ_bar   \u001b[0m\u001b[1m μ_bar_paper \u001b[0m\n",
      "     │\u001b[90m Float64 \u001b[0m\u001b[90m Float64     \u001b[0m\n",
      "─────┼──────────────────────\n",
      "   1 │     4.3          4.3\n",
      "   2 │     7.6          7.6\n",
      "   3 │     9.3          9.3\n",
      "   4 │    11.1         11.0\n",
      "   5 │     4.5          4.5\n",
      "   6 │     7.0          7.0\n",
      "   7 │     8.1          8.1"
     ]
    }
   ],
   "source": [
    "post_est, _ = describe(chain);\n",
    "\n",
    "# print the posterior mu, rounded and in %\n",
    "μ_bar = round.(post_est[:, :mean] * 100, digits=1);\n",
    "\n",
    "μ_bar_exog = get_table(4)[:, :mu_bar];\n",
    "print(DataFrame(:μ_bar => μ_bar, :μ_bar_paper => μ_bar_exog))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bar numerical differences, a perfect match! Now, what about the covariance $\\Sigma_r$? To get to it, we need to sample from the posterior of $r$, which can be achieved by rewriting the function slightly (see the [Turing reference](https://turing.ml/dev/docs/using-turing/guide#treating-observations-as-random-variables) for details):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "blb_with_r (generic function with 2 methods)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# black-litterman bayesian model\n",
    "@model function blb_with_r(q, _r)\n",
    "\n",
    "    global Π, Σ_r, p, ω\n",
    "\n",
    "    if _r === missing\n",
    "        # Initialize `q` if missing\n",
    "        _r = Vector{Float64}(undef, n_assets);\n",
    "    end\n",
    "\n",
    "    # prior on mu\n",
    "    μ_r ~ MvNormal(Π, τ* Σ_r);\n",
    "\n",
    "    # given mu, the views are distributed as:\n",
    "    _q ~ Normal(dot(p, μ_r), sqrt(ω));\n",
    "    \n",
    "    # given mu, the returns are distributed as:\n",
    "    _r ~ MvNormal(μ_r, Σ_r);\n",
    "    \n",
    "    # condition the model on data\n",
    "    return q\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: Found initial step size\n",
      "│   ϵ = 0.0125\n",
      "└ @ Turing.Inference /home/ipozdeev/.julia/packages/Turing/lkUBK/src/mcmc/hmc.jl:191\n"
     ]
    }
   ],
   "source": [
    "# sample from the posterior of r\n",
    "chain = sample(blb_with_r(q, missing), NUTS(), 40000, progress = false);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m6×7 DataFrame\u001b[0m\n",
      "\u001b[1m Row \u001b[0m│\u001b[1m _r[1]   \u001b[0m\u001b[1m _r[2]   \u001b[0m\u001b[1m _r[3]   \u001b[0m\u001b[1m _r[4]   \u001b[0m\u001b[1m _r[5]   \u001b[0m\u001b[1m _r[6]   \u001b[0m\u001b[1m _r[7]   \u001b[0m\n",
      "     │\u001b[90m Float64 \u001b[0m\u001b[90m Float64 \u001b[0m\u001b[90m Float64 \u001b[0m\u001b[90m Float64 \u001b[0m\u001b[90m Float64 \u001b[0m\u001b[90m Float64 \u001b[0m\u001b[90m Float64 \u001b[0m\n",
      "─────┼───────────────────────────────────────────────────────────────\n",
      "   1 │ -0.0614   0.2297  -0.357   -0.0173  -0.2207  -0.201    0.0376\n",
      "   2 │ -0.1253  -0.4605  -0.0232   0.0418   0.1262  -0.0364  -0.1604\n",
      "   3 │ -0.0881   0.033   -0.3467  -0.4152  -0.1394  -0.1314  -0.0899\n",
      "   4 │  0.0078  -0.1789   0.1023   0.0701  -0.1702  -0.1086  -0.0153\n",
      "   5 │ -0.0456  -0.1143  -0.1537   0.0655  -0.2576   0.096   -0.0463\n",
      "   6 │ -0.084   -0.0207   0.2144   0.053   -0.0583  -0.0391   0.12"
     ]
    }
   ],
   "source": [
    "# extract the simulated values from the chain object\n",
    "r_sim = DataFrame(chain)[!, (n_assets+1+3):(n_assets+1+3+n_assets-1)];\n",
    "\n",
    "print(round.(r_sim[1:6, :], digits=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can just estimate the covariance matrix of these values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7×7 Matrix{Float64}:\n",
       " 0.027   0.0169  0.0204  0.0237  0.0155  0.0174  0.0156\n",
       " 0.0169  0.0438  0.0357  0.0384  0.014   0.0263  0.0316\n",
       " 0.0204  0.0357  0.0653  0.0616  0.0194  0.0412  0.0331\n",
       " 0.0237  0.0384  0.0616  0.078   0.021   0.0446  0.0353\n",
       " 0.0155  0.014   0.0194  0.021   0.0462  0.0177  0.0127\n",
       " 0.0174  0.0263  0.0412  0.0446  0.0177  0.0422  0.0259\n",
       " 0.0156  0.0316  0.0331  0.0353  0.0127  0.0259  0.0371"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "Σ_r_post = cov(Matrix(r_sim));\n",
    "\n",
    "display(round.(Σ_r_post, digits=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compared to the original covariance matrix, the posterior is larger in magnitude"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7×7 Matrix{Float64}:\n",
       " 0.0256  0.0159  0.019   0.0223  0.0148  0.0164  0.0147\n",
       " 0.0159  0.0412  0.0334  0.036   0.0132  0.0247  0.0296\n",
       " 0.019   0.0334  0.0615  0.0579  0.0185  0.0388  0.031\n",
       " 0.0223  0.036   0.0579  0.0734  0.0201  0.0421  0.0331\n",
       " 0.0148  0.0132  0.0185  0.0201  0.0441  0.017   0.012\n",
       " 0.0164  0.0247  0.0388  0.0421  0.017   0.04    0.0244\n",
       " 0.0147  0.0296  0.031   0.0331  0.012   0.0244  0.035"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(round.(Σ_r, digits=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "...by &ndash; surprize? &ndash; $(1 + \\tau)$!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7×7 Matrix{Float64}:\n",
       " 1.06  1.07  1.07  1.06  1.05  1.06  1.06\n",
       " 1.07  1.06  1.07  1.07  1.06  1.06  1.07\n",
       " 1.07  1.07  1.06  1.06  1.05  1.06  1.07\n",
       " 1.06  1.07  1.06  1.06  1.04  1.06  1.07\n",
       " 1.05  1.06  1.05  1.04  1.05  1.04  1.05\n",
       " 1.06  1.06  1.06  1.06  1.04  1.06  1.06\n",
       " 1.06  1.07  1.07  1.07  1.05  1.06  1.06"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(round.(Σ_r_post ./ Σ_r, digits=2))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.10.2",
   "language": "julia",
   "name": "julia-1.10"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
